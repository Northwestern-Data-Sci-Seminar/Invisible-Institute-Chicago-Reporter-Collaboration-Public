# -*- coding: utf-8 -*-
"""narratives_tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxWZpqRLe9WKlM0H9wElr8CWDCJbg2XV

Many thanks to this helpful tutorial for guiding us along: https://colab.sandbox.google.com/drive/1JW2I6cU_ypfRXfIfqMPQwMEA6LzGav-Y.

First things first, load data
"""

import pandas as pd

# load data
url_labeled = 'https://raw.githubusercontent.com/Northwestern-Data-Sci-Seminar/Invisible-Institute-Chicago-Reporter-Collaboration-Public/master/The%20Storm%20Panthers/checkpoint-5/src/data/labeled_narratives.csv'
df1_labeled = pd.read_csv(url_labeled) # Dataset is now stored in a Pandas Dataframe
# add column to describe the category
def label_category (row):
  if row['home_invasions'] == 1:
    return 'home_invasion'
  if row['home_invasions'] == 0:
    return 'other'
df1_labeled['category'] = df1_labeled.apply (lambda row: label_category(row), axis=1)
# rename "home_invasions" column
df1_labeled = df1_labeled.rename(columns={"home_invasions": "category_id"})

unlabeled_url = 'https://raw.githubusercontent.com/Northwestern-Data-Sci-Seminar/Invisible-Institute-Chicago-Reporter-Collaboration-Public/master/The%20Storm%20Panthers/checkpoint-5/src/data/unlabeled_narratives.csv'
unlabeled_dfl = pd.read_csv(unlabeled_url) # Dataset is now stored in a Pandas Dataframe

# check it out
df1_labeled

"""Separate the labels out"""

from sklearn.model_selection import train_test_split
import numpy as np

data_labeled = df1_labeled

# separate examples and labels
examples = data_labeled.iloc[:,:-1]
labels = data_labeled['category_id']
print("Check out our examples. We aren't only grabbing the 'summary' column, so keep this in mind for later.")
print(examples)

"""Create the vectors for each summary

* **sublinear_tf=False** means we will not log transform the frequency of the terms.
min_df = 1 (default) means we will not ignore terms that don't meet a certain document frequency.
* **norm is set to l2**, to ensure all our feature vectors have a euclidian norm of 1. This is helpful for visualizing these vectors, and can also improve (or deteriorate) the performance of some models.
* **ngram_range=(1, 3)** means we will consider both unigrams, bigrams, and trigrams.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text

tfidf = TfidfVectorizer(sublinear_tf=False, min_df=1, norm='l2', encoding='latin-1', ngram_range=(1, 3), stop_words='english')
features = tfidf.fit_transform(examples.summary).toarray()

"""We noticed a lot of our FPs when we did our end training were related to 'without justification', 
so we tried removing the word from consideration.

```
# my_stop_words = text.ENGLISH_STOP_WORDS.union(["justification"])
```

It made very little difference,
reducing false positives from 2 to 1 among the test data and only reducing 595 total positives to 589 total positives among the unlabeled data. Thus, we did not move forward with this modification."""

print("Next, we find out most correlated unigrams, bigrams, and trigrams to get a better look at what our labeled data tends to contain.")
print("\n")

from sklearn.feature_selection import chi2
N = 5
features_chi2 = chi2(features, labels)
indices = np.argsort(features_chi2[0])
print(len(indices))
feature_names = np.array(tfidf.get_feature_names())[indices]
unigrams = [v for v in feature_names if len(v.split(' ')) == 1]
bigrams = [v for v in feature_names if len(v.split(' ')) == 2]
trigrams = [v for v in feature_names if len(v.split(' ')) == 3]
print("Most correlated unigrams:\n   . {}".format('\n   . '.join(unigrams[-N:])))
print("Most correlated bigrams:\n   . {}".format('\n   . '.join(bigrams[-N:])))
print("Most correlated trigrams:\n   . {}".format('\n   . '.join(trigrams[-N:])))

"""Let's store some variables to make our lives easier later."""

category_id_df = df1_labeled[['category', 'category_id']].drop_duplicates().sort_values('category_id')
category_to_id = dict(category_id_df.values)
id_to_category = dict(category_id_df[['category_id', 'category']].values)
category_to_id.items()

print("To further paint the picture of what's going on, let's see how the vectors from each of our two categories cluster in relation to each other. We use only the 50 most correlated features to do so, as this helped create a clearer graph after some trial and error.")
print("\n")

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

features_predictive = features[:,indices[-50:]]
projected_features = TSNE(n_components=2, random_state=0).fit_transform(features_predictive)
# print(projected_features)
colors = ['pink', 'green']
for category, category_id in sorted(category_to_id.items()):
    points = projected_features[(labels == category_id).values]
    fig1 = plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)
plt.title("tf-idf feature vector for each summary, projected on 2 dimensions.",
          fontdict=dict(fontsize=15))
plt.legend()
plt.show()

"""Now that we've explored the data a bit, we're ready to build the actual classifier. But before we jump the gun on that, we should test a few different models and see which gives us the best performance. We test a **random forest** classifier, a **multinomial naïve Baye's** classifier, and a **logistic regression** classifier.

For each of these classifiers, we cross validate using five folds. Also, do not fear—it might appear we are forgetting to split training and testing data, but that is actually happening thanks to the `cross_val_score` function.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB

from sklearn.model_selection import cross_val_score


models = [
    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),
    MultinomialNB(),
    LogisticRegression(random_state=0),
]
CV = 5
cv_df = pd.DataFrame(index=range(CV * len(models)))
entries = []
for model in models:
  model_name = model.__class__.__name__
  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)
  for fold_idx, accuracy in enumerate(accuracies):
    entries.append((model_name, fold_idx, accuracy))
cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])

print("Now we can visualize the performance of each model across each of the five folds. The multinomial naïve Bayes classifier clearly wins.")
print("\n")

import seaborn as sns

fig2 = sns.boxplot(x='model_name', y='accuracy', data=cv_df)
fig2b = sns.stripplot(x='model_name', y='accuracy', data=cv_df, 
              size=8, jitter=True, edgecolor="gray", linewidth=2).set_title('Five-fold cross-validation performance across three models')
plt.show()
"""Now we simply take the mean of each of the accuracy at each of the classifiers' folds to get a hard number for the average accuracy of each. We confirm that MultinomialNB performs best with an average accuracy of about 93%."""

cv_df.groupby('model_name').accuracy.mean()

"""Now that we've selected our model, let's train it for real! We do so using an 80/20 training/testing split. We also predict on the test examples and store the results so that we can test the accuracy from this fit."""

from sklearn.model_selection import train_test_split

model = MultinomialNB()

examples_train, examples_test, labels_train, labels_test, indices_train, indices_test = train_test_split(
     features, labels, df1_labeled.index, test_size=0.2, random_state=42)
model.fit(examples_train, labels_train)
y_pred = model.predict(examples_test)

from sklearn.metrics import accuracy_score
print("Accuracy :", accuracy_score(labels_test, y_pred))

print("Sweet, looks pretty good! Let's check out where it's going right and where it's going wrong.")
print("\n")

from sklearn.metrics import confusion_matrix
import seaborn as sns

conf_mat = confusion_matrix(labels_test, y_pred)
fig3 = sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix based on the labeled dataset\'s testing set')
plt.show()

print("Let's actually print out and read the ones that went wrong:")
print("\n")

from IPython.display import display

# print the whole thing without truncation so we can read it:
pd.set_option('display.max_colwidth',1000)

for predicted in category_id_df.category_id:
  for actual in category_id_df.category_id:
    if predicted != actual and conf_mat[actual, predicted] >= 2:
      print("'{}' predicted as '{}' : {} examples.".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))
      display(df1_labeled.loc[indices_test[(labels_test == actual) & (y_pred == predicted)]]['summary'])
      print('')

print("Now we'll do what we did before with the unigrams, bigrams, and trigrams, but with the features our model learned.")
print("\n")

indices = np.argsort(model.coef_[category_id])
feature_names = np.array(tfidf.get_feature_names())[indices]
unigrams = [v for v in feature_names if len(v.split(' ')) == 1]
bigrams = [v for v in feature_names if len(v.split(' ')) == 2]
trigrams = [v for v in feature_names if len(v.split(' ')) == 3]
print("Most correlated unigrams:\n   . {}".format('\n   . '.join(unigrams[-N:])))
print("Most correlated bigrams:\n   . {}".format('\n   . '.join(bigrams[-N:])))
print("Most correlated trigrams:\n   . {}".format('\n   . '.join(trigrams[-N:])))

"""Last but certainly not least, we need to predict the labels of the unlabeled data. We store these so that we can test there accuracy after doing some manual categorical coding."""

print("We see that 495, or 8.3%, of the 5935 unlabeled summaries are labeled as home invasions. We then print out the 20 first summaries labeled as such, and can see from a quick first glance that our accuracy is likely not great.")
print("\n")

unlabeled_features = tfidf.transform(unlabeled_dfl['summary'])
preds = model.predict(unlabeled_features)
print(sum(preds))
print(unlabeled_dfl.shape)
count = 0
for i in range(len(preds)):
  if preds[i] == 1 and count < 20:
    print(unlabeled_dfl.summary[i])
    count += 1

